{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitpj/Exeter-Placement/blob/main/transformersNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 873,
      "metadata": {
        "id": "0zPJfSwCP-cy"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3372244199.py, line 14)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[873], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    import C:/Users/Rohit/Documents/Exeter-Placement/transdim-master/predictor/BTMF.ipynb\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "#sys.path.append('/content/drive/MyDrive/Exeter-Placement/BTMF_original.py')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "from distutils.util import strtobool\n",
        "import sys\n",
        "import BTMF_original\n",
        "import BTMF_Original_Predictor as predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 853,
      "metadata": {
        "id": "BaA4_h2USgB8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\n",
        "    col_names = []\n",
        "    col_types = []\n",
        "    all_data = {}\n",
        "    line_count = 0\n",
        "    frequency = None\n",
        "    forecast_horizon = None\n",
        "    contain_missing_values = None\n",
        "    contain_equal_length = None\n",
        "    found_data_tag = False\n",
        "    found_data_section = False\n",
        "    started_reading_data_section = False\n",
        "\n",
        "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
        "        for line in file:\n",
        "            # Strip white space from start/end of line\n",
        "            line = line.strip()\n",
        "\n",
        "            if line:\n",
        "                if line.startswith(\"@\"):  # Read meta-data\n",
        "                    if not line.startswith(\"@data\"):\n",
        "                        line_content = line.split(\" \")\n",
        "                        if line.startswith(\"@attribute\"):\n",
        "                            if (\n",
        "                                len(line_content) != 3\n",
        "                            ):  # Attributes have both name and type\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            col_names.append(line_content[1])\n",
        "                            col_types.append(line_content[2])\n",
        "                        else:\n",
        "                            if (\n",
        "                                len(line_content) != 2\n",
        "                            ):  # Other meta-data have only values\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            if line.startswith(\"@frequency\"):\n",
        "                                frequency = line_content[1]\n",
        "                            elif line.startswith(\"@horizon\"):\n",
        "                                forecast_horizon = int(line_content[1])\n",
        "                            elif line.startswith(\"@missing\"):\n",
        "                                contain_missing_values = bool(\n",
        "                                    strtobool(line_content[1])\n",
        "                                )\n",
        "                            elif line.startswith(\"@equallength\"):\n",
        "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
        "\n",
        "                    else:\n",
        "                        if len(col_names) == 0:\n",
        "                            raise Exception(\n",
        "                                \"Missing attribute section. Attribute section must come before data.\"\n",
        "                            )\n",
        "\n",
        "                        found_data_tag = True\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    if len(col_names) == 0:\n",
        "                        raise Exception(\n",
        "                            \"Missing attribute section. Attribute section must come before data.\"\n",
        "                        )\n",
        "                    elif not found_data_tag:\n",
        "                        raise Exception(\"Missing @data tag.\")\n",
        "                    else:\n",
        "                        if not started_reading_data_section:\n",
        "                            started_reading_data_section = True\n",
        "                            found_data_section = True\n",
        "                            all_series = []\n",
        "\n",
        "                            for col in col_names:\n",
        "                                all_data[col] = []\n",
        "\n",
        "                        full_info = line.split(\":\")\n",
        "\n",
        "                        if len(full_info) != (len(col_names) + 1):\n",
        "                            raise Exception(\"Missing attributes/values in series.\")\n",
        "\n",
        "                        series = full_info[len(full_info) - 1]\n",
        "                        series = series.split(\",\")\n",
        "\n",
        "                        if len(series) == 0:\n",
        "                            raise Exception(\n",
        "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
        "                            )\n",
        "\n",
        "                        numeric_series = []\n",
        "\n",
        "                        for val in series:\n",
        "                            if val == \"?\":\n",
        "                                numeric_series.append(replace_missing_vals_with)\n",
        "                            else:\n",
        "                                numeric_series.append(float(val))\n",
        "\n",
        "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
        "                            numeric_series\n",
        "                        ):\n",
        "                            raise Exception(\n",
        "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
        "                            )\n",
        "\n",
        "                        all_series.append(pd.Series(numeric_series).array)\n",
        "\n",
        "                        for i in range(len(col_names)):\n",
        "                            att_val = None\n",
        "                            if col_types[i] == \"numeric\":\n",
        "                                att_val = int(full_info[i])\n",
        "                            elif col_types[i] == \"string\":\n",
        "                                att_val = str(full_info[i])\n",
        "                            elif col_types[i] == \"date\":\n",
        "                                att_val = datetime.strptime(\n",
        "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
        "                                )\n",
        "                            else:\n",
        "                                raise Exception(\n",
        "                                    \"Invalid attribute type.\"\n",
        "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
        "\n",
        "                            if att_val is None:\n",
        "                                raise Exception(\"Invalid attribute value.\")\n",
        "                            else:\n",
        "                                all_data[col_names[i]].append(att_val)\n",
        "\n",
        "                line_count = line_count + 1\n",
        "\n",
        "        if line_count == 0:\n",
        "            raise Exception(\"Empty file.\")\n",
        "        if len(col_names) == 0:\n",
        "            raise Exception(\"Missing attribute section.\")\n",
        "        if not found_data_section:\n",
        "            raise Exception(\"Missing series information under data section.\")\n",
        "\n",
        "        all_data[value_column_name] = all_series\n",
        "        loaded_data = pd.DataFrame(all_data)\n",
        "\n",
        "        return (\n",
        "            loaded_data,\n",
        "            frequency,\n",
        "            forecast_horizon,\n",
        "            contain_missing_values,\n",
        "            contain_equal_length,\n",
        "        )\n",
        "# Example of usage\n",
        "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 854,
      "metadata": {
        "id": "XNtA88ETUVQ0"
      },
      "outputs": [],
      "source": [
        "def trim_dataframe(filename,columnname):\n",
        "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(filename)\n",
        "    building_data = loaded_data[loaded_data['series_name'].str.contains(columnname)]\n",
        "    max_start_timestamp = building_data['start_timestamp'].max()\n",
        "    building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
        "    min_timestamps = building_data['num_timestamps'].min()\n",
        "    building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
        "    building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
        "    return building_data['uniform_series']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 855,
      "metadata": {
        "id": "y-YamZO0UXMn"
      },
      "outputs": [],
      "source": [
        "def impute_dataframe(dataframe, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\"):\n",
        "    dense_tensor = dataframe\n",
        "    dim = dense_tensor.shape\n",
        "    list_of_arrays = [np.array(series) for series in dense_tensor]\n",
        "    # Stack these arrays vertically to form a 2D matrix\n",
        "    dense_mat_2d = np.vstack(list_of_arrays)\n",
        "    sparse_mat = dense_mat_2d.copy()\n",
        "    dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
        "    sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
        "    del dense_tensor\n",
        "    dim1, dim2 = sparse_mat.shape\n",
        "    init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
        "    mat, W, X, A= BTMF_original.BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)\n",
        "    # Assuming you have column names and indices stored\n",
        "    df=pd.DataFrame(mat)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 856,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ubX1fu3UaW1",
        "outputId": "fa7f80bb-bc6d-4b06-db5f-a42f831c40c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_20716\\1322639957.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_20716\\1322639957.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
            "C:\\Users\\Rohit\\AppData\\Local\\Temp\\ipykernel_20716\\1322639957.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BTMF Iteration: 0\n"
          ]
        }
      ],
      "source": [
        "NaN_df=trim_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\",\"Building\")\n",
        "time_lags = np.array([1, 4, 96])\n",
        "burn_iter=0\n",
        "gibbs_iter=1\n",
        "rank=10\n",
        "\n",
        "df=impute_dataframe(NaN_df,rank,time_lags,burn_iter,gibbs_iter)\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df.T).T, columns=df.columns, index=df.index)\n",
        "\n",
        "sequence_length = 8640  # 3 months of data\n",
        "prediction_length = 2880  # 1 month of data\n",
        "\n",
        "train_end = 41572 - prediction_length - sequence_length\n",
        "train_data = df_scaled.iloc[:, :train_end]\n",
        "validation_data = df_scaled.iloc[:, train_end:train_end+sequence_length]\n",
        "test_data = df_scaled.iloc[:, train_end+sequence_length:train_end+2*sequence_length]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 857,
      "metadata": {
        "id": "y52n-XhaUc06"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, seq_length, pred_length):\n",
        "    sequences = []\n",
        "    target_sequences = []\n",
        "\n",
        "    for i in range(len(data.columns) - seq_length - pred_length + 1):\n",
        "        sequences.append(data.iloc[:, i:i+seq_length].values)\n",
        "        target_sequences.append(data.iloc[:, i+seq_length:i+seq_length+pred_length].values)\n",
        "\n",
        "    return np.array(sequences), np.array(target_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 858,
      "metadata": {
        "id": "MNtl0uZ4Ud6Q"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = create_sequences(train_data, sequence_length, prediction_length)\n",
        "X_val, Y_val = create_sequences(validation_data, sequence_length, prediction_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 859,
      "metadata": {
        "id": "VVv08J8AUgET"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_enc = np.zeros((d_model, seq_length))\n",
        "    pos_enc[0::2, :] = np.sin(position * div_term).T\n",
        "    pos_enc[1::2, :] = np.cos(position * div_term).T\n",
        "    return pos_enc[np.newaxis, :, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 860,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzduyfWeUgkh",
        "outputId": "b17c9cd9-d341-4117-bad6-849c91c55e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 6, 8640) <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "d_model = 6  # Number of sensors\n",
        "# Generate positional encoding\n",
        "pos_enc = positional_encoding(sequence_length, d_model)\n",
        "print(pos_enc.shape, type(pos_enc))\n",
        "# Add positional encoding to X_train\n",
        "X_val = validation_data.values[np.newaxis, :, :]\n",
        "X_train += pos_enc\n",
        "X_val += pos_enc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 861,
      "metadata": {
        "id": "3AMz8PHvU2sA"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        \"\"\"\n",
        "        self.wq = nn.Linear(8640, d_model)\n",
        "        self.wk = nn.Linear(8640, d_model)\n",
        "        self.wv = nn.Linear(8640, d_model)\n",
        "        \"\"\"\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
        "        d_k = self.depth ** 0.5\n",
        "        scaled_attention_logits = matmul_qk / d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.dense(output), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 862,
      "metadata": {
        "id": "d19EaOAcVJdI"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "          nn.Linear(d_model, dff),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 863,
      "metadata": {
        "id": "_y5fZmsoVMy6"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(8640, 512)\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create a positional encoding tensor with a sequence length of 8640\n",
        "        #self.pos_encoding = torch.tensor(positional_encoding(8640, 512), dtype=torch.float32)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, training, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        #print(\"batch size\",batch_size,\"seq length\",seq_len)\n",
        "        #print(f\"X Initial shape: {x.shape}\")\n",
        "        \n",
        "        # Reshape and apply input_proj\n",
        "        x = x.view(-1, 8640)\n",
        "        #print(f\"X After first reshape: {x.shape}\")\n",
        "        x = self.input_proj(x)\n",
        "        #print(f\"X After input_proj: {x.shape}\")\n",
        "        x = x.view(batch_size, seq_len, -1)  # Use the model dimension for reshaping\n",
        "        #print(f\"X After second reshape: {x.shape}\")\n",
        "\n",
        "        #self.pos_encoding = torch.tensor(positional_encoding(seq_len, self.d_model), dtype=torch.float32)\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(self.d_model,seq_len), dtype=torch.float32)\n",
        "\n",
        "        #print(\"  pos encoding shape\",self.pos_encoding.shape)\n",
        "        self.pos_encoding = self.pos_encoding.clone().detach().to(x.device)\n",
        "        #print(\"  pos encoding shape after dupe\",self.pos_encoding.shape)       \n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "        # Print shapes before adding positional encoding\n",
        "        #print(f\"Shape of x: {x.shape}\")\n",
        "        #print(f\"Shape of positional encoding: {self.pos_encoding.shape}\")\n",
        "\n",
        "        # Add positional encoding\n",
        "        #add=self.pos_encoding[:, :seq_len, :]\n",
        "        add=self.pos_encoding\n",
        "        \n",
        "        #print(\"add shape\", add.shape)\n",
        "        x += add\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        #print(\"applying input projection\")\n",
        "        #print(\"final x shape\", x.shape)\n",
        "        for i in range(self.num_layers):\n",
        "            #print(f\"Properties of EncoderLayer {i}:\")\n",
        "            for name, param in self.enc_layers[i].named_parameters():\n",
        "                pass\n",
        "                #print(f\"Parameter Name: {name}, Shape: {param.shape}\")\n",
        "            \n",
        "            x = self.enc_layers[i](x, mask)\n",
        "            #print(\" layer\",i,\"applied\")\n",
        "            #print(\"X after encoder layer \",i, \" \", x.shape)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 864,
      "metadata": {
        "id": "A3nin5F6V7_0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\"\"\"\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1)  # Remove training=training\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2)  # Remove training=training\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)  # Remove training=training\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 865,
      "metadata": {
        "id": "oM2quFbkV9vw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(2880, 512)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(max_position_encoding, d_model), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        attention_weights = {}\n",
        "        print(f\"Decoder X Initial shape: {x.shape}\")\n",
        "        # Remove the embedding line\n",
        "        # x = self.embedding(x) * torch.sqrt(torch.tensor(self.d_model))\n",
        "\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        #x += self.pos_encoding[:, :seq_len].to(x.device)\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        x += self.pos_encoding[:, :seq_len, :x.size(2)]\n",
        "        print(f\"Decoder X after pos encoding: {x.shape}\")\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "            print(\"X after decoder layer \",i, \" \", x.shape)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_position_encoding, dropout_rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_proj = nn.Linear(2880, 512)\n",
        "        self.output_proj = nn.Linear(512, 2880)  # Add this line\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoding = torch.tensor(positional_encoding(max_position_encoding, d_model), dtype=torch.float32)\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = x.size(1)\n",
        "        attention_weights = {}\n",
        "        #print(f\"Decoder X Initial shape: {x.shape}\")\n",
        "\n",
        "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        x += self.pos_encoding[:, :seq_len, :x.size(2)]\n",
        "        #print(f\"Decoder X after pos encoding: {x.shape}\")\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "            #print(\"X after decoder layer \",i, \" \", x.shape)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        x = self.output_proj(x)  # Add this line\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 866,
      "metadata": {
        "id": "UAr55WK-V_kv"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, dropout_rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout_rate)\n",
        "\n",
        "        #self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "        self.final_layer = nn.Linear(512, 2880)\n",
        "\n",
        "        #self.final_layer = nn.Linear(2880, target_vocab_size)\n",
        "    def forward(self, inp, tar, training=True, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)\n",
        "        #dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "        #print(\"Shape of dec_output:\", dec_output.shape)\n",
        "        final_output = dec_output\n",
        "        #final_output = self.final_layer(dec_output)\n",
        "        #print(\"final output shape: \", final_output.shape)\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 867,
      "metadata": {
        "id": "CHNB_7c3Wqwf"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "input_vocab_size = 512\n",
        "target_vocab_size = 512\n",
        "max_position_encoding_input = 9000  # A bit more than 8640 to ensure coverage\n",
        "max_position_encoding_target = 9000\n",
        "\n",
        "# Create the Transformer model instance\n",
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,\n",
        "                    pe_input=max_position_encoding_input, pe_target=max_position_encoding_target)\n",
        "model.train()\n",
        "# Now, you can define your optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 868,
      "metadata": {
        "id": "Y-7Az3tkWIK1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 869,
      "metadata": {
        "id": "_3ofdPV0b1fW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 870,
      "metadata": {
        "id": "wbGk5J7Gb-Dv"
      },
      "outputs": [],
      "source": [
        "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
        "                    input_vocab_size=512, target_vocab_size=512,\n",
        "                    pe_input=9000, pe_target=9000)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 871,
      "metadata": {
        "id": "7_HkKUhuWIz3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<enumerate object at 0x000001E0EBC537E0>\n",
            "batch num  0\n",
            "batch num  50\n",
            "batch num  100\n",
            "batch num  150\n",
            "batch num  200\n",
            "batch num  250\n",
            "batch num  300\n",
            "batch num  350\n",
            "batch num  400\n",
            "batch num  450\n",
            "batch num  500\n",
            "batch num  550\n",
            "total loss:  7593.478533744812\n",
            "Epoch [1/2], Loss: 13.0922\n",
            "<enumerate object at 0x000001E0EBCA0EF0>\n",
            "batch num  0\n",
            "batch num  50\n",
            "batch num  100\n",
            "batch num  150\n",
            "batch num  200\n",
            "batch num  250\n",
            "batch num  300\n",
            "batch num  350\n",
            "batch num  400\n",
            "batch num  450\n",
            "batch num  500\n",
            "batch num  550\n",
            "total loss:  7515.914110183716\n",
            "Epoch [2/2], Loss: 12.9585\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(enumerate(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if batch_idx %50==0:\n",
        "            print(\"batch num \", batch_idx)\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        #print(\"Input data shape:\", data.shape)\n",
        "        target = target.float()\n",
        "\n",
        "        # Forward pass\n",
        "        # Exclude the last time step from target as input to the decoder\n",
        "        outputs, _ = model(data, target[:, :-1])\n",
        "        #print(\"computing loss\")\n",
        "        # Compute the loss\n",
        "        # Exclude the first time step from target to match the shape of outputs\n",
        "        loss = criterion(outputs, target[:, 1:])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        #print(\"backward pass\")\n",
        "        loss.backward()\n",
        "        #print(\"optimization\")\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(\"total loss: \", total_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3k8vaospFic"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd5pXQdZpvkv"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOiPQZglxoKZu7/sIBv+Lop",
      "include_colab_link": true,
      "mount_file_id": "1fP-ZxX6dalSO3A7vKNed7Co6n_zBHeqA",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
