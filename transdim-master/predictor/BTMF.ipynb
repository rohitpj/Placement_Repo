{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Revised**: October 8, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/predictor/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Matrix Factorization (BTMF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTMF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTMF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "We assume a spatiotemporal setting for multidimensional time series data throughout this work. In general, modern spatiotemporal data sets collected from sensor networks can be organized as matrix time series. For example, we can denote by matrix $Y\\in\\mathbb{R}^{N\\times T}$ a multivariate time series collected from $N$ locations/sensors on $T$ time points, with each row $$\\boldsymbol{y}_{i}=\\left(y_{i,1},y_{i,2},...,y_{i,t-1},y_{i,t},y_{i,t+1},...,y_{i,T}\\right)$$\n",
    "corresponding to the time series collected at location $i$.\n",
    "\n",
    "As mentioned, making accurate predictions on incomplete time series is very challenging, while missing data problem is almost inevitable in real-world applications. Figure 1 illustrates the prediction problem for incomplete time series data. Here we use $(i,t)\\in\\Omega$ to index the observed entries in matrix $Y$.\n",
    "\n",
    "<img src=\"../images/graphical_matrix_time_series.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "> **Figure 1**: Illustration of multivariate time series and the prediction problem in the presence of missing values (green: observed data; white: missing data; red: prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"hello world\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(temp * W_bar, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)\n",
    "\n",
    "def sample_precision_scalar_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat)\n",
    "    return np.sum(np.abs(var[valid_indices] - var_hat[valid_indices]) / var[valid_indices]) / valid_indices.sum()\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat)\n",
    "    return np.sqrt(np.sum((var[valid_indices] - var_hat[valid_indices]) ** 2) / valid_indices.sum())\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "def compute_mape(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat) & (var != 0)\n",
    "    if valid_indices.sum() == 0:  # Check if there are valid indices\n",
    "        return np.nan\n",
    "    return np.sum(np.abs(var[valid_indices] - var_hat[valid_indices]) / var[valid_indices]) / valid_indices.sum()\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    valid_indices = ~np.isnan(var) & ~np.isnan(var_hat)\n",
    "    if valid_indices.sum() == 0:  # Check if there are valid indices\n",
    "        return np.nan\n",
    "    return np.sqrt(np.sum((var[valid_indices] - var_hat[valid_indices]) ** 2) / valid_indices.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar4cast(A, X, Sigma, time_lags, multi_step):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    X_new = np.append(X, np.zeros((multi_step, rank)), axis = 0)\n",
    "    for t in range(multi_step):\n",
    "        var = A.T @ X_new[dim + t - time_lags, :].reshape(rank * d)\n",
    "        X_new[dim + t, :] = mvnrnd(var, Sigma)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTMF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1, option = \"factor\"):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "        pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        pos_test = np.where((dense_mat != 0) & (np.isnan(sparse_mat)))\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank, gibbs_iter))\n",
    "    A_plus = np.zeros((rank * d, rank, gibbs_iter))\n",
    "    tau_plus = np.zeros((dim1, gibbs_iter))\n",
    "    Sigma_plus = np.zeros((rank, rank, gibbs_iter))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 500\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    X_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        if option == \"factor\":\n",
    "            tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        elif option == \"pca\":\n",
    "            tau = sample_precision_scalar_tau(sparse_mat, mat_hat, ind)\n",
    "            tau = tau * np.ones(dim1)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus[:, :, it - burn_iter] = W\n",
    "            A_plus[:, :, it - burn_iter] = A\n",
    "            Sigma_plus[:, :, it - burn_iter] = Sigma\n",
    "            tau_plus[:, it - burn_iter] = tau\n",
    "            mat_hat_plus += mat_hat\n",
    "            X0 = ar4cast(A, X, Sigma, time_lags, multi_step)\n",
    "            X_plus[:, :, it - burn_iter] = X0\n",
    "            mat_new_plus += W @ X0[dim2 : dim2 + multi_step, :].T\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    #print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    #print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print()\n",
    "    mat_hat = np.append(mat_hat, mat_new_plus / gibbs_iter, axis = 1)\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_plus, A_plus, Sigma_plus, tau_plus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x, back_step):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind[:, - back_step :]).reshape([rank, rank, back_step]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat[:, - back_step :]\n",
    "    for t in range(dim2 - back_step, dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        var3[:, :, t + back_step - dim2] = var3[:, :, t + back_step - dim2] + Mt\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t + back_step - dim2], \n",
    "                                   var4[:, t + back_step - dim2] + Nt + Qt), var3[:, :, t + back_step - dim2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF_partial(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1, gamma = 10):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    W_plus = init[\"W_plus\"]\n",
    "    X_plus = init[\"X_plus\"]\n",
    "    A_plus = init[\"A_plus\"]\n",
    "    Sigma_plus = init[\"Sigma_plus\"]\n",
    "    tau_plus = init[\"tau_plus\"]\n",
    "    if np.isnan(sparse_mat).any() == False:\n",
    "        ind = sparse_mat != 0\n",
    "        pos_obs = np.where(ind)\n",
    "    elif np.isnan(sparse_mat).any() == True:\n",
    "        ind = ~np.isnan(sparse_mat)\n",
    "        pos_obs = np.where(ind)\n",
    "        sparse_mat[np.isnan(sparse_mat)] = 0\n",
    "    X_new_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    back_step = gamma * multi_step\n",
    "    for it in range(gibbs_iter):\n",
    "        tau_ind = tau_plus[:, it][:, None] * ind\n",
    "        tau_sparse_mat = tau_plus[:, it][:, None] * sparse_mat\n",
    "        X = sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W_plus[:, :, it], \n",
    "                                    X_plus[:, :, it], A_plus[:, :, it], inv(Sigma_plus[:, :, it]), back_step)\n",
    "        X0 = ar4cast(A_plus[:, :, it], X, Sigma_plus[:, :, it], time_lags, multi_step)\n",
    "        X_new_plus[:, :, it] = X0\n",
    "        mat_new_plus += W_plus[:, :, it] @ X0[- multi_step :, :].T\n",
    "    mat_hat = mat_new_plus / gibbs_iter\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_new_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef new_BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \\n                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\\n    dim1, T = dense_mat.shape\\n    start_time = T - pred_step\\n    max_count = int(np.ceil(pred_step / multi_step))\\n    mat_hat = np.zeros((dim1, max_count * multi_step))\\n    mape_values=[]\\n    rmse_values=[]\\n    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\\n    #display(f) # display the bar\\n    for t in range(max_count):\\n        if t == 0:\\n            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\\n            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \\n                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\\n        else:\\n            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\\n            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \\n                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \\n                burn_iter, gibbs_iter, multi_step, gamma)\\n            small_dense_mat = dense_mat[:, start_time : T]\\n            pos = np.where(small_dense_mat != 0)\\n            mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n            mape_value_noisy = add_noise_to_loss(mape_value)\\n            mape_values.append(mape_value)\\n            rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n            rmse_value_noisy = add_noise_to_loss(rmse_value)\\n            rmse_values.append(rmse_value)\\n        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\\n        #f.value = t\\n    small_dense_mat = dense_mat[:, start_time : T]\\n    pos = np.where(small_dense_mat != 0)\\n    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n    print(\\'Prediction MAPE: {:.6}\\'.format(mape_value))\\n    print(\\'Prediction RMSE: {:.6}\\'.format(rmse_value))\\n    print()\\n    return mat_hat, mape_values, rmse_values\\n\\ndef BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \\n                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\\n    dim1, T = dense_mat.shape\\n    start_time = T - pred_step\\n    max_count = int(np.ceil(pred_step / multi_step))\\n    mat_hat = np.zeros((dim1, max_count * multi_step))\\n    mape_values=[]\\n    rmse_values=[]\\n    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\\n    #display(f) # display the bar\\n    for t in range(max_count):\\n        if t == 0:\\n            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\\n            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \\n                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\\n        else:\\n            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\\n            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \\n                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \\n                burn_iter, gibbs_iter, multi_step, gamma)\\n            small_dense_mat = dense_mat[:, start_time : T]\\n            pos = np.where(small_dense_mat != 0)\\n            mape_loss = compute_mape(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\\n            mape_values.append(mape_loss)\\n            rmse_loss = compute_rmse(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\\n            rmse_values.append(rmse_loss)\\n        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\\n        #f.value = t\\n    small_dense_mat = dense_mat[:, start_time : T]\\n    pos = np.where(small_dense_mat != 0)\\n    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n    print(\\'Prediction MAPE: {:.6}\\'.format(mape_value))\\n    print(\\'Prediction RMSE: {:.6}\\'.format(rmse_value))\\n    print()\\n    return mat_hat, mape_values, rmse_values\\n'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    mape_values=[]\n",
    "    rmse_values=[]\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "            small_dense_mat = dense_mat[:, start_time : T]\n",
    "            pos = np.where(small_dense_mat != 0)\n",
    "            mape_loss = compute_mape(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            mape_values.append(mape_loss)\n",
    "            rmse_loss = compute_rmse(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            rmse_values.append(rmse_loss)\n",
    "            if t%50 == 0:\n",
    "                print('Prediction MAPE: {:.6}'.format(mape_loss))\n",
    "                print('Prediction RMSE: {:.6}'.format(rmse_loss))\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "    print('Prediction MAPE: {:.6}'.format(mape_value))\n",
    "    print('Prediction RMSE: {:.6}'.format(rmse_value))\n",
    "    print()\n",
    "    return mat_hat, mape_values, rmse_values\n",
    "\"\"\"\n",
    "\n",
    "def new_BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \n",
    "                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    mape_values=[]\n",
    "    rmse_values=[]\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "            small_dense_mat = dense_mat[:, start_time : T]\n",
    "            pos = np.where(small_dense_mat != 0)\n",
    "            mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "            mape_value_noisy = add_noise_to_loss(mape_value)\n",
    "            mape_values.append(mape_value)\n",
    "            rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "            rmse_value_noisy = add_noise_to_loss(rmse_value)\n",
    "            rmse_values.append(rmse_value)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "    print('Prediction MAPE: {:.6}'.format(mape_value))\n",
    "    print('Prediction RMSE: {:.6}'.format(rmse_value))\n",
    "    print()\n",
    "    return mat_hat, mape_values, rmse_values\n",
    "\n",
    "def BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \n",
    "                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    mape_values=[]\n",
    "    rmse_values=[]\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "            small_dense_mat = dense_mat[:, start_time : T]\n",
    "            pos = np.where(small_dense_mat != 0)\n",
    "            mape_loss = compute_mape(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            mape_values.append(mape_loss)\n",
    "            rmse_loss = compute_rmse(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            rmse_values.append(rmse_loss)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "    print('Prediction MAPE: {:.6}'.format(mape_value))\n",
    "    print('Prediction RMSE: {:.6}'.format(rmse_value))\n",
    "    print()\n",
    "    return mat_hat, mape_values, rmse_values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n",
    "\n",
    "# print(loaded_data)\n",
    "# print(frequency)\n",
    "# print(forecast_horizon)\n",
    "# print(contain_missing_values)\n",
    "# print(contain_equal_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   series_name     start_timestamp  \\\n",
      "0    Building0 2016-07-03 21:30:00   \n",
      "1    Building1 2019-01-09 23:15:00   \n",
      "2    Building3 2016-03-01 04:15:00   \n",
      "3    Building4 2019-07-03 04:45:00   \n",
      "4    Building5 2019-07-25 23:00:00   \n",
      "5    Building6 2019-07-25 01:45:00   \n",
      "6       Solar0 2020-04-25 14:00:00   \n",
      "7       Solar1 2018-12-31 13:00:00   \n",
      "8       Solar2 2019-06-05 14:00:00   \n",
      "9       Solar3 2019-06-05 14:00:00   \n",
      "10      Solar4 2019-06-05 14:00:00   \n",
      "11      Solar5 2019-01-15 13:00:00   \n",
      "\n",
      "                                         series_value  \n",
      "0   [283.8, 283.8, 283.8, 606.0, 606.0, 606.0, 606...  \n",
      "1   [8.1, 15.7, 22.8, 32.7, 8.1, 16.5, 24.7, 34.5,...  \n",
      "2   [1321.0, 1321.0, 1321.0, 1321.0, 1293.0, 1293....  \n",
      "3   [2.0, NaN, 1.0, 2.0, NaN, 2.0, NaN, NaN, 2.0, ...  \n",
      "4   [30.0, 31.0, 24.0, 34.0, 30.0, 31.0, 26.0, 33....  \n",
      "5   [36.8, 34.6, 34.6, 36.2, 36.2, 35.2, 35.2, 35....  \n",
      "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 3. Calculate the number of 15-minute intervals to trim for each series\\nbuilding_data['intervals_to_trim'] = building_data['start_timestamp'].apply(lambda x: int((x - max_start_timestamp).total_seconds() // (15*60)))\\n\\n# 4. Trim the time series of each building from the beginning up to this timestamp\\nbuilding_data['trimmed_series'] = building_data.apply(lambda row: row['series_value'][row['intervals_to_trim']:], axis=1)\\n\\n# 5. Determine the minimum length of the trimmed series\\nmin_length = building_data['trimmed_series'].apply(len).min()\\n\\n# 6. Trim (or pad) all series to this minimum length\\nbuilding_data['uniform_series'] = building_data['trimmed_series'].apply(lambda x: x[:min_length])\\n\\n#print(building_data)\\n#print(building_data['uniform_series'].shape)\\n\\n# 7. Convert the uniform series to a matrix format\\nmatrix = np.array(building_data['uniform_series'].tolist())\\n\""
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the .tsf file\n",
    "\n",
    "# Convert to DataFrame\n",
    "\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "\n",
    "print(loaded_data)\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "building_data = loaded_data[loaded_data['series_name'].str.contains('Building')]\n",
    "\n",
    "max_start_timestamp = building_data['start_timestamp'].max()\n",
    "\n",
    "building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
    "\n",
    "min_timestamps = building_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# 3. Calculate the number of 15-minute intervals to trim for each series\n",
    "building_data['intervals_to_trim'] = building_data['start_timestamp'].apply(lambda x: int((x - max_start_timestamp).total_seconds() // (15*60)))\n",
    "\n",
    "# 4. Trim the time series of each building from the beginning up to this timestamp\n",
    "building_data['trimmed_series'] = building_data.apply(lambda row: row['series_value'][row['intervals_to_trim']:], axis=1)\n",
    "\n",
    "# 5. Determine the minimum length of the trimmed series\n",
    "min_length = building_data['trimmed_series'].apply(len).min()\n",
    "\n",
    "# 6. Trim (or pad) all series to this minimum length\n",
    "building_data['uniform_series'] = building_data['trimmed_series'].apply(lambda x: x[:min_length])\n",
    "\n",
    "#print(building_data)\n",
    "#print(building_data['uniform_series'].shape)\n",
    "\n",
    "# 7. Convert the uniform series to a matrix format\n",
    "matrix = np.array(building_data['uniform_series'].tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Assuming loaded_data is already prepared and is in the format of a dense matrix\\ndense_mat = building_data[\\'uniform_series\\']  # Adjust this based on how you\\'ve prepared loaded_data\\n\\n  # Assuming you want to work on a copy of the original data\\n\\nlist_of_arrays = [np.array(series) for series in dense_mat]\\nmape_values = []\\nrmse_values = []\\n# Stack these arrays vertically to form a 2D matrix\\ndense_mat_2d = np.vstack(list_of_arrays)\\nsparse_mat = dense_mat_2d.copy()\\nprint(\"dense mat shape\",dense_mat_2d.shape)\\nprint(dense_mat_2d)\\ndense_mat_2d = np.where(dense_mat_2d == \\'NaN\\', np.nan, dense_mat_2d).astype(float)\\nsparse_mat = np.where(sparse_mat == \\'NaN\\', np.nan, sparse_mat).astype(float)\\n\\n# Model Setting\\nrank = 20\\npred_step = 2976 \\ntime_lags = np.array([1, 4, 96])  \\n#time_lags = np.array([1, 96, 384])  # Adjust this based on the seasonality or patterns in your data#\\nburn_iter = 20\\ngibbs_iter = 5\\nmulti_step = 1\\ndim1, dim2 = sparse_mat.shape\\ninit = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\\nprint(\"starting prediction\")\\n# Apply BTMF forecasting for different prediction time horizons (if needed)\\n\\n\\nstart = time.time()\\nprint(\\'Prediction time horizon (delta) = {}.\\'.format(multi_step))\\nbuilding_mat_hat, old_mape_values, old_rmse_values = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\\nplt.plot(old_mape_values, marker=\\'o\\', color=\\'red\\')\\nplt.title(\\'Loss over Iterations\\')\\nplt.xlabel(\\'Iteration\\')\\nplt.ylabel(\\'MAPE Loss)\\')\\nplt.grid(True)\\nfilename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/mape_plot_\" + dt.datetime.now().strftime(\\'%Y%m%d_%H%M%S\\') + \".png\"\\nplt.savefig(filename)\\nplt.show()\\n\\nplt.plot(old_rmse_values, marker=\\'o\\', color=\\'red\\')\\nplt.title(\\'Loss over Iterations\\')\\nplt.xlabel(\\'Iteration\\')\\nplt.ylabel(\\'RMSE Loss)\\')\\nplt.grid(True)\\nfilename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/rmse_plot_\" + dt.datetime.now().strftime(\\'%Y%m%d_%H%M%S\\') + \".png\"\\nplt.savefig(filename)\\nplt.show()\\n\\nprint(building_mat_hat)\\nend = time.time()\\nprint(\\'Running time: %d seconds\\'%(end - start))\\nprint()\\nprint(building_mat_hat.shape)\\nplt.figure(figsize=(10, 5))\\n\\n\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime as dt\n",
    "\"\"\"\n",
    "# Assuming loaded_data is already prepared and is in the format of a dense matrix\n",
    "dense_mat = building_data['uniform_series']  # Adjust this based on how you've prepared loaded_data\n",
    "\n",
    "  # Assuming you want to work on a copy of the original data\n",
    "\n",
    "list_of_arrays = [np.array(series) for series in dense_mat]\n",
    "mape_values = []\n",
    "rmse_values = []\n",
    "# Stack these arrays vertically to form a 2D matrix\n",
    "dense_mat_2d = np.vstack(list_of_arrays)\n",
    "sparse_mat = dense_mat_2d.copy()\n",
    "print(\"dense mat shape\",dense_mat_2d.shape)\n",
    "print(dense_mat_2d)\n",
    "dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
    "sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
    "\n",
    "# Model Setting\n",
    "rank = 20\n",
    "pred_step = 2976 \n",
    "time_lags = np.array([1, 4, 96])  \n",
    "#time_lags = np.array([1, 96, 384])  # Adjust this based on the seasonality or patterns in your data#\n",
    "burn_iter = 20\n",
    "gibbs_iter = 5\n",
    "multi_step = 1\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "print(\"starting prediction\")\n",
    "# Apply BTMF forecasting for different prediction time horizons (if needed)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "building_mat_hat, old_mape_values, old_rmse_values = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "plt.plot(old_mape_values, marker='o', color='red')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MAPE Loss)')\n",
    "plt.grid(True)\n",
    "filename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/mape_plot_\" + dt.datetime.now().strftime('%Y%m%d_%H%M%S') + \".png\"\n",
    "plt.savefig(filename)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(old_rmse_values, marker='o', color='red')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE Loss)')\n",
    "plt.grid(True)\n",
    "filename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/rmse_plot_\" + dt.datetime.now().strftime('%Y%m%d_%H%M%S') + \".png\"\n",
    "plt.savefig(filename)\n",
    "plt.show()\n",
    "\n",
    "print(building_mat_hat)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "print()\n",
    "print(building_mat_hat.shape)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\n# Read the .tsf file\\n\\n# Convert to DataFrame\\nloaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\\n#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\\nsolar_data = loaded_data[loaded_data[\\'series_name\\'].str.contains(\\'Solar\\')]\\n#print(building_data)\\n# 2. Determine the maximum starting timestamp among the buildings\\n\\nmax_start_timestamp = solar_data[\\'start_timestamp\\'].max()\\n#print(\"max timestamp\",max_start_timestamp)\\nsolar_data[\\'num_timestamps\\'] = solar_data[\\'series_value\\'].apply(len)\\n#print(solar_data[\\'num_timestamps\\'])\\nmin_timestamps = solar_data[\\'num_timestamps\\'].min()\\n\\n# Trim each time series to have the same length as the shortest one\\nsolar_data[\\'uniform_series\\'] = solar_data[\\'series_value\\'].apply(lambda x: x[-min_timestamps:])\\n\\n# Update the \\'num_timestamps\\' column to reflect the new uniform length\\nsolar_data[\\'num_timestamps\\'] = solar_data[\\'uniform_series\\'].apply(len)\\n#print(solar_data[\\'uniform_series\\'])\\n'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the .tsf file\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Read the .tsf file\n",
    "\n",
    "# Convert to DataFrame\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "solar_data = loaded_data[loaded_data['series_name'].str.contains('Solar')]\n",
    "#print(building_data)\n",
    "# 2. Determine the maximum starting timestamp among the buildings\n",
    "\n",
    "max_start_timestamp = solar_data['start_timestamp'].max()\n",
    "#print(\"max timestamp\",max_start_timestamp)\n",
    "solar_data['num_timestamps'] = solar_data['series_value'].apply(len)\n",
    "#print(solar_data['num_timestamps'])\n",
    "min_timestamps = solar_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "solar_data['uniform_series'] = solar_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "solar_data['num_timestamps'] = solar_data['uniform_series'].apply(len)\n",
    "#print(solar_data['uniform_series'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport scipy.io\\nimport numpy as np\\nimport time\\nimport datetime\\nprint(\"test starting\")\\n# Convert to DataFrame\\nfrom datetime import datetime\\nfrom distutils.util import strtobool\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\ndef convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\\n    col_names = []\\n    col_types = []\\n    all_data = {}\\n    line_count = 0\\n    frequency = None\\n    forecast_horizon = None\\n    contain_missing_values = None\\n    contain_equal_length = None\\n    found_data_tag = False\\n    found_data_section = False\\n    started_reading_data_section = False\\n\\n    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\\n        for line in file:\\n            # Strip white space from start/end of line\\n            line = line.strip()\\n\\n            if line:\\n                if line.startswith(\"@\"):  # Read meta-data\\n                    if not line.startswith(\"@data\"):\\n                        line_content = line.split(\" \")\\n                        if line.startswith(\"@attribute\"):\\n                            if (\\n                                len(line_content) != 3\\n                            ):  # Attributes have both name and type\\n                                raise Exception(\"Invalid meta-data specification.\")\\n\\n                            col_names.append(line_content[1])\\n                            col_types.append(line_content[2])\\n                        else:\\n                            if (\\n                                len(line_content) != 2\\n                            ):  # Other meta-data have only values\\n                                raise Exception(\"Invalid meta-data specification.\")\\n\\n                            if line.startswith(\"@frequency\"):\\n                                frequency = line_content[1]\\n                            elif line.startswith(\"@horizon\"):\\n                                forecast_horizon = int(line_content[1])\\n                            elif line.startswith(\"@missing\"):\\n                                contain_missing_values = bool(\\n                                    strtobool(line_content[1])\\n                                )\\n                            elif line.startswith(\"@equallength\"):\\n                                contain_equal_length = bool(strtobool(line_content[1]))\\n\\n                    else:\\n                        if len(col_names) == 0:\\n                            raise Exception(\\n                                \"Missing attribute section. Attribute section must come before data.\"\\n                            )\\n\\n                        found_data_tag = True\\n                elif not line.startswith(\"#\"):\\n                    if len(col_names) == 0:\\n                        raise Exception(\\n                            \"Missing attribute section. Attribute section must come before data.\"\\n                        )\\n                    elif not found_data_tag:\\n                        raise Exception(\"Missing @data tag.\")\\n                    else:\\n                        if not started_reading_data_section:\\n                            started_reading_data_section = True\\n                            found_data_section = True\\n                            all_series = []\\n\\n                            for col in col_names:\\n                                all_data[col] = []\\n\\n                        full_info = line.split(\":\")\\n\\n                        if len(full_info) != (len(col_names) + 1):\\n                            raise Exception(\"Missing attributes/values in series.\")\\n\\n                        series = full_info[len(full_info) - 1]\\n                        series = series.split(\",\")\\n\\n                        if len(series) == 0:\\n                            raise Exception(\\n                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\\n                            )\\n\\n                        numeric_series = []\\n\\n                        for val in series:\\n                            if val == \"?\":\\n                                numeric_series.append(replace_missing_vals_with)\\n                            else:\\n                                numeric_series.append(float(val))\\n\\n                        if numeric_series.count(replace_missing_vals_with) == len(\\n                            numeric_series\\n                        ):\\n                            raise Exception(\\n                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\\n                            )\\n\\n                        all_series.append(pd.Series(numeric_series).array)\\n\\n                        for i in range(len(col_names)):\\n                            att_val = None\\n                            if col_types[i] == \"numeric\":\\n                                att_val = int(full_info[i])\\n                            elif col_types[i] == \"string\":\\n                                att_val = str(full_info[i])\\n                            elif col_types[i] == \"date\":\\n                                att_val = datetime.strptime(\\n                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\\n                                )\\n                            else:\\n                                raise Exception(\\n                                    \"Invalid attribute type.\"\\n                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\\n\\n                            if att_val is None:\\n                                raise Exception(\"Invalid attribute value.\")\\n                            else:\\n                                all_data[col_names[i]].append(att_val)\\n\\n                line_count = line_count + 1\\n\\n        if line_count == 0:\\n            raise Exception(\"Empty file.\")\\n        if len(col_names) == 0:\\n            raise Exception(\"Missing attribute section.\")\\n        if not found_data_section:\\n            raise Exception(\"Missing series information under data section.\")\\n\\n        all_data[value_column_name] = all_series\\n        loaded_data = pd.DataFrame(all_data)\\n\\n        return (\\n            loaded_data,\\n            frequency,\\n            forecast_horizon,\\n            contain_missing_values,\\n            contain_equal_length,\\n        )\\n\\nfrom IPython.display import display\\ndef new_BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \\n                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\\n    dim1, T = dense_mat.shape\\n    start_time = T - pred_step\\n    max_count = int(np.ceil(pred_step / multi_step))\\n    mat_hat = np.zeros((dim1, max_count * multi_step))\\n    mape_values=[]\\n    rmse_values=[]\\n    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\\n    #display(f) # display the bar\\n    for t in range(max_count):\\n        if t == 0:\\n            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\\n            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \\n                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\\n        else:\\n            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\\n            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \\n                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \\n                burn_iter, gibbs_iter, multi_step, gamma)\\n            small_dense_mat = dense_mat[:, start_time : T]\\n            pos = np.where(small_dense_mat != 0)\\n            mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n            mape_value_noisy = add_noise_to_loss(mape_value)\\n            mape_values.append(mape_value)\\n            rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n            rmse_value_noisy = add_noise_to_loss(rmse_value)\\n            rmse_values.append(rmse_value)\\n        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\\n        #f.value = t\\n    small_dense_mat = dense_mat[:, start_time : T]\\n    pos = np.where(small_dense_mat != 0)\\n    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n    print(\\'Prediction MAPE: {:.6}\\'.format(mape_value))\\n    print(\\'Prediction RMSE: {:.6}\\'.format(rmse_value))\\n    print()\\n    return mat_hat, mape_values, rmse_values\\n\\ndef BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \\n                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\\n    dim1, T = dense_mat.shape\\n    start_time = T - pred_step\\n    max_count = int(np.ceil(pred_step / multi_step))\\n    mat_hat = np.zeros((dim1, max_count * multi_step))\\n    mape_values=[]\\n    rmse_values=[]\\n    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\\n    #display(f) # display the bar\\n    for t in range(max_count):\\n        if t == 0:\\n            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\\n            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \\n                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\\n        else:\\n            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\\n            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \\n                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \\n                burn_iter, gibbs_iter, multi_step, gamma)\\n            small_dense_mat = dense_mat[:, start_time : T]\\n            pos = np.where(small_dense_mat != 0)\\n            mape_loss = compute_mape(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\\n            mape_values.append(mape_loss)\\n            rmse_loss = compute_rmse(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\\n            rmse_values.append(rmse_loss)\\n        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\\n        #f.value = t\\n    small_dense_mat = dense_mat[:, start_time : T]\\n    pos = np.where(small_dense_mat != 0)\\n    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\\n    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\\n    print(\\'Prediction MAPE: {:.6}\\'.format(mape_value))\\n    print(\\'Prediction RMSE: {:.6}\\'.format(rmse_value))\\n    print()\\n    return mat_hat, mape_values, rmse_values\\n\\nloaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\\n\\nprint(loaded_data)\\n#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\\nsolar_data = loaded_data[loaded_data[\\'series_name\\'].str.contains(\\'solar\\')]\\n\\nmax_start_timestamp = solar_data[\\'start_timestamp\\'].max()\\n\\nsolar_data[\\'num_timestamps\\'] = solar_data[\\'series_value\\'].apply(len)\\n\\nmin_timestamps = solar_data[\\'num_timestamps\\'].min()\\n\\n# Trim each time series to have the same length as the shortest one\\nsolar_data[\\'uniform_series\\'] = solar_data[\\'series_value\\'].apply(lambda x: x[-min_timestamps:])\\n\\n# Update the \\'num_timestamps\\' column to reflect the new uniform length\\nsolar_data[\\'num_timestamps\\'] = solar_data[\\'uniform_series\\'].apply(len)\\n\\n# Assuming loaded_data is already prepared and is in the format of a dense matrix\\ndense_mat = solar_data[\\'uniform_series\\']  # Adjust this based on how you\\'ve prepared loaded_data\\n\\nloaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\\n\\nprint(loaded_data)\\n#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\\nbuilding_data = loaded_data[loaded_data[\\'series_name\\'].str.contains(\\'Solar\\')]\\n\\nmax_start_timestamp = building_data[\\'start_timestamp\\'].max()\\n\\nbuilding_data[\\'num_timestamps\\'] = building_data[\\'series_value\\'].apply(len)\\n\\nmin_timestamps = building_data[\\'num_timestamps\\'].min()\\n\\n# Trim each time series to have the same length as the shortest one\\nbuilding_data[\\'uniform_series\\'] = building_data[\\'series_value\\'].apply(lambda x: x[-min_timestamps:])\\n\\n# Update the \\'num_timestamps\\' column to reflect the new uniform length\\nbuilding_data[\\'num_timestamps\\'] = building_data[\\'uniform_series\\'].apply(len)\\n  # Assuming you want to work on a copy of the original data\\n\\nlist_of_arrays = [np.array(series) for series in dense_mat]\\nmape_values = []\\nrmse_values = []\\n# Stack these arrays vertically to form a 2D matrix\\ndense_mat_2d = np.vstack(list_of_arrays)\\nsparse_mat = dense_mat_2d.copy()\\nprint(\"dense mat shape\",dense_mat_2d.shape)\\nprint(dense_mat_2d)\\ndense_mat_2d = np.where(dense_mat_2d == \\'NaN\\', np.nan, dense_mat_2d).astype(float)\\nsparse_mat = np.where(sparse_mat == \\'NaN\\', np.nan, sparse_mat).astype(float)\\n\\n# Model Setting\\nrank = 10\\npred_step = 2976 \\ntime_lags = np.array([1, 4, 96])  \\n#time_lags = np.array([1, 96, 384])  # Adjust this based on the seasonality or patterns in your data#\\nburn_iter = 50\\ngibbs_iter = 12\\nmulti_step = 1\\ndim1, dim2 = sparse_mat.shape\\ninit = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\\nprint(\"starting prediction\")\\n# Apply BTMF forecasting for different prediction time horizons (if needed)\\n\\nstart = time.time()\\nprint(\\'Prediction time horizon (delta) = {}.\\'.format(multi_step))\\nbuilding_mat_hat, mape_values, rmse_values = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\\nplt.plot(mape_values, marker=\\'o\\', color=\\'red\\')\\nplt.title(\\'Loss over Iterations\\')\\nplt.xlabel(\\'Iteration\\')\\nplt.ylabel(\\'MAPE Loss)\\')\\nplt.grid(True)\\nfilename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/mape_plot_\" + datetime.datetime.now().strftime(\\'%Y%m%d_%H%M%S\\') + \".png\"\\nplt.savefig(filename)\\nplt.show()\\n\\nplt.plot(rmse_values, marker=\\'o\\', color=\\'red\\')\\nplt.title(\\'Loss over Iterations\\')\\nplt.xlabel(\\'Iteration\\')\\nplt.ylabel(\\'RMSE Loss)\\')\\nplt.grid(True)\\nfilename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/rmse_plot_\" + datetime.datetime.now().strftime(\\'%Y%m%d_%H%M%S\\') + \".png\"\\nplt.savefig(filename)\\nplt.show()\\n\\nprint(building_mat_hat)\\nend = time.time()\\nprint(\\'Running time: %d seconds\\'%(end - start))\\nprint()\\nprint(building_mat_hat.shape)\\nplt.figure(figsize=(10, 5))\\n\\n\\nplt.tight_layout()\\nplt.show()\\n\\nimport scipy.io\\nimport numpy as np\\n\\n# Assuming loaded_data is already prepared and is in the format of a dense matrix\\ndense_mat = solar_data[\\'uniform_series\\']  # Adjust this based on how you\\'ve prepared loaded_data\\n\\n  # Assuming you want to work on a copy of the original data\\n\\nlist_of_arrays = [np.array(series) for series in dense_mat]\\n\\n# Stack these arrays vertically to form a 2D matrix\\ndense_mat_2d = np.vstack(list_of_arrays)\\nsparse_mat = dense_mat_2d.copy()\\nprint(\"dense mat shape\",dense_mat_2d.shape)\\nprint(dense_mat_2d)\\ndense_mat_2d = np.where(dense_mat_2d == \\'NaN\\', np.nan, dense_mat_2d).astype(float)\\nsparse_mat = np.where(sparse_mat == \\'NaN\\', np.nan, sparse_mat).astype(float)\\n\\n# Model Setting\\nrank = 10\\npred_step = 2976  # You mentioned you want to predict the next 248 energy demand values\\ntime_lags = np.array([1, 4, 96])  # Adjust this based on the seasonality or patterns in your data#\\nburn_iter = 2\\ngibbs_iter = 1\\ndim1, dim2 = sparse_mat.shape\\ninit = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\\n\\nBTMFburn=2\\nBTMFgibbs=1\\nmatrix, W_plus, X_plus, A_plus, Sigma_plus, tau_plus=BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, BTMFburn, BTMFgibbs, multi_step = 1, option = \"factor\")\\nprint(\"matrix shape: \", matrix.shape)\\nprint(matrix)\\n\\ndf = pd.DataFrame(matrix)  # Assuming mat_hat is your data matrix\\ndf.to_csv(\\'C:/Users/Rohit/Documents/Exeter-Placement/Results/solar_predicted_data.csv\\', index=False)\\n\\nprint(\"starting prediction\")\\n# Apply BTMF forecasting for different prediction time horizons (if needed)\\nfor multi_step in [4, 24, 48, 96, 672]:  # Adjust this list based on your requirements\\n    start = time.time()\\n    print(\\'Prediction time horizon (delta) = {}.\\'.format(multi_step))\\n    solar_mat_hat = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\\n    print(solar_mat_hat)\\n    end = time.time()\\n    print(\\'Running time: %d seconds\\'%(end - start))\\n    print()\\nprint(solar_mat_hat.shape)\\n'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "print(\"test starting\")\n",
    "# Convert to DataFrame\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\",):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "from IPython.display import display\n",
    "def new_BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \n",
    "                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    mape_values=[]\n",
    "    rmse_values=[]\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "            small_dense_mat = dense_mat[:, start_time : T]\n",
    "            pos = np.where(small_dense_mat != 0)\n",
    "            mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "            mape_value_noisy = add_noise_to_loss(mape_value)\n",
    "            mape_values.append(mape_value)\n",
    "            rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "            rmse_value_noisy = add_noise_to_loss(rmse_value)\n",
    "            rmse_values.append(rmse_value)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "    print('Prediction MAPE: {:.6}'.format(mape_value))\n",
    "    print('Prediction RMSE: {:.6}'.format(rmse_value))\n",
    "    print()\n",
    "    return mat_hat, mape_values, rmse_values\n",
    "\n",
    "def BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, \n",
    "                  rank, time_lags, burn_iter, gibbs_iter, option = \"factor\", gamma = 10):\n",
    "    dim1, T = dense_mat.shape\n",
    "    start_time = T - pred_step\n",
    "    max_count = int(np.ceil(pred_step / multi_step))\n",
    "    mat_hat = np.zeros((dim1, max_count * multi_step))\n",
    "    mape_values=[]\n",
    "    rmse_values=[]\n",
    "    #f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    #display(f) # display the bar\n",
    "    for t in range(max_count):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step, option)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                burn_iter, gibbs_iter, multi_step, gamma)\n",
    "            small_dense_mat = dense_mat[:, start_time : T]\n",
    "            pos = np.where(small_dense_mat != 0)\n",
    "            mape_loss = compute_mape(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            mape_values.append(mape_loss)\n",
    "            rmse_loss = compute_rmse(small_dense_mat[pos], mat_hat[pos])  # or compute_mape, based on your preference\n",
    "            rmse_values.append(rmse_loss)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        #f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : T]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    mape_value = compute_mape(small_dense_mat[pos], mat_hat[pos])\n",
    "    rmse_value = compute_rmse(small_dense_mat[pos], mat_hat[pos])\n",
    "    print('Prediction MAPE: {:.6}'.format(mape_value))\n",
    "    print('Prediction RMSE: {:.6}'.format(rmse_value))\n",
    "    print()\n",
    "    return mat_hat, mape_values, rmse_values\n",
    "\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "\n",
    "print(loaded_data)\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "solar_data = loaded_data[loaded_data['series_name'].str.contains('solar')]\n",
    "\n",
    "max_start_timestamp = solar_data['start_timestamp'].max()\n",
    "\n",
    "solar_data['num_timestamps'] = solar_data['series_value'].apply(len)\n",
    "\n",
    "min_timestamps = solar_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "solar_data['uniform_series'] = solar_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "solar_data['num_timestamps'] = solar_data['uniform_series'].apply(len)\n",
    "\n",
    "# Assuming loaded_data is already prepared and is in the format of a dense matrix\n",
    "dense_mat = solar_data['uniform_series']  # Adjust this based on how you've prepared loaded_data\n",
    "\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf\")\n",
    "\n",
    "print(loaded_data)\n",
    "#print(loaded_data.shape,frequency, forecast_horizon, contain_missing_values, contain_equal_length)\n",
    "building_data = loaded_data[loaded_data['series_name'].str.contains('Solar')]\n",
    "\n",
    "max_start_timestamp = building_data['start_timestamp'].max()\n",
    "\n",
    "building_data['num_timestamps'] = building_data['series_value'].apply(len)\n",
    "\n",
    "min_timestamps = building_data['num_timestamps'].min()\n",
    "\n",
    "# Trim each time series to have the same length as the shortest one\n",
    "building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])\n",
    "\n",
    "# Update the 'num_timestamps' column to reflect the new uniform length\n",
    "building_data['num_timestamps'] = building_data['uniform_series'].apply(len)\n",
    "  # Assuming you want to work on a copy of the original data\n",
    "\n",
    "list_of_arrays = [np.array(series) for series in dense_mat]\n",
    "mape_values = []\n",
    "rmse_values = []\n",
    "# Stack these arrays vertically to form a 2D matrix\n",
    "dense_mat_2d = np.vstack(list_of_arrays)\n",
    "sparse_mat = dense_mat_2d.copy()\n",
    "print(\"dense mat shape\",dense_mat_2d.shape)\n",
    "print(dense_mat_2d)\n",
    "dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
    "sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
    "\n",
    "# Model Setting\n",
    "rank = 10\n",
    "pred_step = 2976 \n",
    "time_lags = np.array([1, 4, 96])  \n",
    "#time_lags = np.array([1, 96, 384])  # Adjust this based on the seasonality or patterns in your data#\n",
    "burn_iter = 50\n",
    "gibbs_iter = 12\n",
    "multi_step = 1\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "print(\"starting prediction\")\n",
    "# Apply BTMF forecasting for different prediction time horizons (if needed)\n",
    "\n",
    "start = time.time()\n",
    "print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "building_mat_hat, mape_values, rmse_values = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "plt.plot(mape_values, marker='o', color='red')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MAPE Loss)')\n",
    "plt.grid(True)\n",
    "filename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/mape_plot_\" + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + \".png\"\n",
    "plt.savefig(filename)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rmse_values, marker='o', color='red')\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE Loss)')\n",
    "plt.grid(True)\n",
    "filename = \"C:/Users/Rohit/Documents/Exeter-Placement/Results/rmse_plot_\" + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + \".png\"\n",
    "plt.savefig(filename)\n",
    "plt.show()\n",
    "\n",
    "print(building_mat_hat)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "print()\n",
    "print(building_mat_hat.shape)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Assuming loaded_data is already prepared and is in the format of a dense matrix\n",
    "dense_mat = solar_data['uniform_series']  # Adjust this based on how you've prepared loaded_data\n",
    "\n",
    "  # Assuming you want to work on a copy of the original data\n",
    "\n",
    "list_of_arrays = [np.array(series) for series in dense_mat]\n",
    "\n",
    "# Stack these arrays vertically to form a 2D matrix\n",
    "dense_mat_2d = np.vstack(list_of_arrays)\n",
    "sparse_mat = dense_mat_2d.copy()\n",
    "print(\"dense mat shape\",dense_mat_2d.shape)\n",
    "print(dense_mat_2d)\n",
    "dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)\n",
    "sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)\n",
    "\n",
    "# Model Setting\n",
    "rank = 10\n",
    "pred_step = 2976  # You mentioned you want to predict the next 248 energy demand values\n",
    "time_lags = np.array([1, 4, 96])  # Adjust this based on the seasonality or patterns in your data#\n",
    "burn_iter = 2\n",
    "gibbs_iter = 1\n",
    "dim1, dim2 = sparse_mat.shape\n",
    "init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(dim2, rank)}\n",
    "\n",
    "BTMFburn=2\n",
    "BTMFgibbs=1\n",
    "matrix, W_plus, X_plus, A_plus, Sigma_plus, tau_plus=BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, BTMFburn, BTMFgibbs, multi_step = 1, option = \"factor\")\n",
    "print(\"matrix shape: \", matrix.shape)\n",
    "print(matrix)\n",
    "\n",
    "df = pd.DataFrame(matrix)  # Assuming mat_hat is your data matrix\n",
    "df.to_csv('C:/Users/Rohit/Documents/Exeter-Placement/Results/solar_predicted_data.csv', index=False)\n",
    "\n",
    "print(\"starting prediction\")\n",
    "# Apply BTMF forecasting for different prediction time horizons (if needed)\n",
    "for multi_step in [4, 24, 48, 96, 672]:  # Adjust this list based on your requirements\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    solar_mat_hat = BTMF_forecast(dense_mat_2d, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    print(solar_mat_hat)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()\n",
    "print(solar_mat_hat.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
