import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime
from distutils.util import strtobool
import sys
import BTMF_original
import torch
import torch.nn as nn
import torch.nn.functional as F
import tensorflow as tf
from tensorflow.keras.layers import Layer

# Parameters
# full_file_path_and_name - complete .tsf file path
# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe
# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe
def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with="NaN", value_column_name="series_value",):
    col_names = []
    col_types = []
    all_data = {}
    line_count = 0
    frequency = None
    forecast_horizon = None
    contain_missing_values = None
    contain_equal_length = None
    found_data_tag = False
    found_data_section = False
    started_reading_data_section = False

    with open(full_file_path_and_name, "r", encoding="cp1252") as file:
        for line in file:
            # Strip white space from start/end of line
            line = line.strip()

            if line:
                if line.startswith("@"):  # Read meta-data
                    if not line.startswith("@data"):
                        line_content = line.split(" ")
                        if line.startswith("@attribute"):
                            if (
                                len(line_content) != 3
                            ):  # Attributes have both name and type
                                raise Exception("Invalid meta-data specification.")

                            col_names.append(line_content[1])
                            col_types.append(line_content[2])
                        else:
                            if (
                                len(line_content) != 2
                            ):  # Other meta-data have only values
                                raise Exception("Invalid meta-data specification.")

                            if line.startswith("@frequency"):
                                frequency = line_content[1]
                            elif line.startswith("@horizon"):
                                forecast_horizon = int(line_content[1])
                            elif line.startswith("@missing"):
                                contain_missing_values = bool(
                                    strtobool(line_content[1])
                                )
                            elif line.startswith("@equallength"):
                                contain_equal_length = bool(strtobool(line_content[1]))

                    else:
                        if len(col_names) == 0:
                            raise Exception(
                                "Missing attribute section. Attribute section must come before data."
                            )

                        found_data_tag = True
                elif not line.startswith("#"):
                    if len(col_names) == 0:
                        raise Exception(
                            "Missing attribute section. Attribute section must come before data."
                        )
                    elif not found_data_tag:
                        raise Exception("Missing @data tag.")
                    else:
                        if not started_reading_data_section:
                            started_reading_data_section = True
                            found_data_section = True
                            all_series = []

                            for col in col_names:
                                all_data[col] = []

                        full_info = line.split(":")

                        if len(full_info) != (len(col_names) + 1):
                            raise Exception("Missing attributes/values in series.")

                        series = full_info[len(full_info) - 1]
                        series = series.split(",")

                        if len(series) == 0:
                            raise Exception(
                                "A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol"
                            )

                        numeric_series = []

                        for val in series:
                            if val == "?":
                                numeric_series.append(replace_missing_vals_with)
                            else:
                                numeric_series.append(float(val))

                        if numeric_series.count(replace_missing_vals_with) == len(
                            numeric_series
                        ):
                            raise Exception(
                                "All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series."
                            )

                        all_series.append(pd.Series(numeric_series).array)

                        for i in range(len(col_names)):
                            att_val = None
                            if col_types[i] == "numeric":
                                att_val = int(full_info[i])
                            elif col_types[i] == "string":
                                att_val = str(full_info[i])
                            elif col_types[i] == "date":
                                att_val = datetime.strptime(
                                    full_info[i], "%Y-%m-%d %H-%M-%S"
                                )
                            else:
                                raise Exception(
                                    "Invalid attribute type."
                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.

                            if att_val is None:
                                raise Exception("Invalid attribute value.")
                            else:
                                all_data[col_names[i]].append(att_val)

                line_count = line_count + 1

        if line_count == 0:
            raise Exception("Empty file.")
        if len(col_names) == 0:
            raise Exception("Missing attribute section.")
        if not found_data_section:
            raise Exception("Missing series information under data section.")

        all_data[value_column_name] = all_series
        loaded_data = pd.DataFrame(all_data)

        return (
            loaded_data,
            frequency,
            forecast_horizon,
            contain_missing_values,
            contain_equal_length,
        )
# Example of usage
# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe("TSForecasting/tsf_data/sample.tsf")

def trim_dataframe(filename,columnname):
    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(filename)
    building_data = loaded_data[loaded_data['series_name'].str.contains(columnname)]
    max_start_timestamp = building_data['start_timestamp'].max()
    building_data['num_timestamps'] = building_data['series_value'].apply(len)
    min_timestamps = building_data['num_timestamps'].min()
    building_data['uniform_series'] = building_data['series_value'].apply(lambda x: x[-min_timestamps:])
    building_data['num_timestamps'] = building_data['uniform_series'].apply(len)
    return building_data['uniform_series']

def impute_dataframe(dataframe, rank, time_lags, burn_iter, gibbs_iter, option = "factor"):
    dense_tensor = dataframe
    dim = dense_tensor.shape
    list_of_arrays = [np.array(series) for series in dense_tensor]
    # Stack these arrays vertically to form a 2D matrix
    dense_mat_2d = np.vstack(list_of_arrays)
    sparse_mat = dense_mat_2d.copy()
    dense_mat_2d = np.where(dense_mat_2d == 'NaN', np.nan, dense_mat_2d).astype(float)
    sparse_mat = np.where(sparse_mat == 'NaN', np.nan, sparse_mat).astype(float)
    del dense_tensor
    dim1, dim2 = sparse_mat.shape
    init = {"W": 0.1 * np.random.randn(dim1, rank), "X": 0.1 * np.random.randn(dim2, rank)}
    mat, W, X, A= BTMF_original.BTMF(dense_mat_2d, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter)
    # Assuming you have column names and indices stored
    df=pd.DataFrame(mat)
    return df

NaN_df=trim_dataframe("C:/Users/Rohit/Documents/Exeter-Placement/Challenge/phase_1 data/phase_1_data/phase_1_data.tsf","Building")
time_lags = np.array([1, 4, 96])  
burn_iter=0
gibbs_iter=1
rank=10

df=impute_dataframe(NaN_df,rank,time_lags,burn_iter,gibbs_iter)
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df.T).T, columns=df.columns, index=df.index)

sequence_length = 8640  # 3 months of data
prediction_length = 2880  # 1 month of data

train_end = 41572 - prediction_length - sequence_length
train_data = df_scaled.iloc[:, :train_end]
validation_data = df_scaled.iloc[:, train_end:train_end+sequence_length]
test_data = df_scaled.iloc[:, train_end+sequence_length:train_end+2*sequence_length]

def create_sequences(data, seq_length, pred_length):
    sequences = []
    target_sequences = []
    
    for i in range(len(data.columns) - seq_length - pred_length + 1):
        sequences.append(data.iloc[:, i:i+seq_length].values)
        target_sequences.append(data.iloc[:, i+seq_length:i+seq_length+pred_length].values)
    
    return np.array(sequences), np.array(target_sequences)

X_train, y_train = create_sequences(train_data, sequence_length, prediction_length)
X_val, y_val = create_sequences(validation_data, sequence_length, prediction_length)

def positional_encoding(seq_length, d_model):
    position = np.arange(seq_length)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    pos_enc = np.zeros((d_model, seq_length))
    pos_enc[0::2, :] = np.sin(position * div_term).T
    pos_enc[1::2, :] = np.cos(position * div_term).T
    return pos_enc[np.newaxis, :, :]

d_model = 6  # Number of sensors
# Generate positional encoding
pos_enc = positional_encoding(sequence_length, d_model)
# Add positional encoding to X_train
X_val = validation_data.values[np.newaxis, :, :]
X_train += pos_enc
X_val += pos_enc

